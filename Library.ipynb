{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import KMeans\n",
    "import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_main(maxiter):\n",
    "    \n",
    "    #Initialize\n",
    "    X, X_population = KMeans_initialize()\n",
    "    k = determine_K(X)\n",
    "    \n",
    "    #Defining centers\n",
    "    centroids = np.asarray([[1.0, 1.0], [-1.0, -1.0], [1.0, -1.0]])\n",
    "    \n",
    "    # Initialize the vectors in which we will store the assigned classes of each data point and the calculated distances from each centroid\n",
    "    classes = np.zeros(X.shape[0], dtype=np.float64)\n",
    "    distances = np.zeros([X.shape[0], k], dtype=np.float64)\n",
    "\n",
    "    # start_kmeans = time.time()\n",
    "    centroids, classes = MyKMeans(maxiter, centroids, classes, distances, X, k)\n",
    "    # end_kmeans = time.time()\n",
    "    #print(centroids)\n",
    "    \n",
    "    accuracy_kmeans = validate_Kmeans(X, centroids)\n",
    "    #print(accuracy_kmeans)\n",
    "\n",
    "    #Time taken in seconds\n",
    "    # time_kmeans = end_kmeans - start_kmeans\n",
    "    # print(\"Elapsed (after compilation) = %s\" % time_kmeans)\n",
    "    \n",
    "    return accuracy_kmeans, k, X_population, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_DBSCAN(skl_labels, my_labels):\n",
    "    # Scikit learn uses -1 to for NOISE, and starts cluster labeling at 0. I start numbering at 1, so increment the skl cluster numbers by 1.\n",
    "    for i in range(0, len(skl_labels)):\n",
    "        if not skl_labels[i] == -1:\n",
    "            skl_labels[i] += 1\n",
    "\n",
    "    num_disagree = 0\n",
    "    # Go through each label and make sure they match (print the labels if they # don't)\n",
    "    count = 0\n",
    "    for i in range(0, len(skl_labels)):\n",
    "        if not skl_labels[i] == my_labels[i]:\n",
    "            print('Scikit learn:', skl_labels[i], 'mine:', my_labels[i])\n",
    "            num_disagree += 1\n",
    "        else:\n",
    "            count = count+1\n",
    "    return num_disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCAN_main(X):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # start_dbscan = time.time()\n",
    "    dictionary = {\"if_mydbscan\":0, \"else_mydbscan\":0, \"else_gc_body_subordinate_if\":0, \"else_gc\":0}\n",
    "    my_labels, dictionary = MyDBSCAN(X, dictionary, eps=0.3, MinPts=10)\n",
    "    # end_dbscan = time.time()\n",
    "\n",
    "    # time_dbscan = end_dbscan - start_dbscan\n",
    "    # print(\"Elapsed (after compilation) = %s\" % (time_dbscan))\n",
    "    \n",
    "\n",
    "    db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "    skl_labels = db.labels_\n",
    "\n",
    "    num_disagree = validate_DBSCAN(skl_labels, my_labels) \n",
    "    return num_disagree, dictionary\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_cost(N, dictionary):\n",
    "    sub_cost = 1\n",
    "    add_cost = 1\n",
    "    mul_cost = 2\n",
    "    div_cost = 2\n",
    "    comp_lt_cost = 1\n",
    "    comp_eq_cost = 1\n",
    "    comp_get_cost = 1\n",
    "    comp_neq_cost = 1\n",
    "    comp_gteq_cost = 1\n",
    "\n",
    "    region_query_cost = N * (sub_cost+mul_cost+comp_lt_cost)\n",
    "    else_gc_body_cost = comp_eq_cost + region_query_cost + comp_gteq_cost + 0 + (dictionary[\"else_gc_body_subordinate_if\"] * add_cost)\n",
    "    grow_cluster_cost = N * ((comp_lt_cost + comp_eq_cost + add_cost) + 0 + (dictionary[\"else_gc\"] * else_gc_body_cost))\n",
    "    dbscsn_cost = N * (comp_neq_cost + region_query_cost + \n",
    "    ( (dictionary[\"if_mydbscan\"] * (comp_eq_cost+comp_lt_cost)) + (dictionary[\"else_mydbscan\"] * (add_cost+comp_eq_cost+comp_lt_cost+grow_cluster_cost)) ) )\n",
    "\n",
    "    \n",
    "    return dbscsn_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cost(N):\n",
    "    # we define costs for certain operations so that it will eventually contribute to the net cost computation\n",
    "        # N is determined at run time so it is passed as a parameter\n",
    "    sub_cost = 1\n",
    "    add_cost = 1\n",
    "    mul_cost = 2\n",
    "    div_cost = 2\n",
    "\n",
    "    kmeans_cost = N * (maxiter * k * (2 * sub_cost + add_cost + 2 * mul_cost) + 1 + k)\n",
    "    return kmeans_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stats_dict(filename):\n",
    "        cache_stats = {}\n",
    "        with open(filename, \"r\") as input_file:\n",
    "            for line in input_file:\n",
    "                data = line.split(\"== \")[1]\n",
    "                data = data.strip().split(\":\")\n",
    "                if len(data[0]) > 0:\n",
    "                    key = data[0]\n",
    "                    val = data[1].strip().split(\" \")[0]\n",
    "                    cache_stats.setdefault(key, val)\n",
    "        return cache_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_analysis(length, dictionary, k, maxiter):\n",
    "    KMeans_cost = kmeans_cost(length)\n",
    "    print(\"KMeans cost = \", KMeans_cost)\n",
    "\n",
    "    DBSCAN_cost = dbscan_cost(length, dictionary)\n",
    "    print(\"DBSCAN_cost = \", DBSCAN_cost)\n",
    "    \n",
    "    cmd = \"valgrind --tool=cachegrind python3 KMeans.py \"+ str(k) +\" \"+ str(maxiter) + \" 1>&temp\"\n",
    "    !{cmd}\n",
    "    cmd = \"tail -n16 temp>&input_kmeans\"\n",
    "    !{cmd}\n",
    "    \n",
    "    cmd = \"valgrind --tool=cachegrind python3 DBSCAN.py 1>&temp\"\n",
    "    !{cmd}\n",
    "    cmd = \"tail -n16 temp>&input_dbscan\"\n",
    "    !{cmd}\n",
    "     \n",
    "    cache_stats_kmeans = build_stats_dict(\"input_kmeans\")\n",
    "    print(cache_stats_kmeans)\n",
    "    cache_stats_dbscan = build_stats_dict(\"input_dbscan\")\n",
    "    print(cache_stats_dbscan)\n",
    "        \n",
    "    return KMeans_cost, DBSCAN_cost, cache_stats_kmeans, cache_stats_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_choice(KMeans_cost, DBSCAN_cost, X_population, k, maxiter):\n",
    "    X= X_population\n",
    "    if(KMeans_cost > DBSCAN_cost):\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        my_labels = MyDBSCAN(X, eps=0.3, MinPts=10)\n",
    "    else:\n",
    "                \n",
    "        classes = np.zeros(X.shape[0], dtype=np.float64)\n",
    "        distances = np.zeros([X.shape[0], k], dtype=np.float64)\n",
    "        centroids = np.asarray([[1.0, 1.0], [-1.0, -1.0], [1.0, -1.0]]) #KMeans\n",
    "                \n",
    "        centroids, classes = MyKMeans(maxiter, centroids, classes, distances, X_population, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99686907  1.00400708]\n",
      " [-1.00468252 -1.00079739]\n",
      " [ 0.99888398 -0.99877808]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    maxiter = 50\n",
    "    accuracy_kmeans, k, X_population, X = KMeans_main(maxiter)\n",
    "    \n",
    "    if accuracy_kmeans == 1:\n",
    "        \n",
    "        X= X_population\n",
    "\n",
    "        centroids = np.asarray([[1.0, 1.0], [-1.0, -1.0], [1.0, -1.0]]) #KMeans\n",
    "        classes = np.zeros(X.shape[0], dtype=np.float64)\n",
    "        distances = np.zeros([X.shape[0], k], dtype=np.float64)\n",
    "        \n",
    "        centroids, classes = MyKMeans(maxiter, centroids, classes, distances, X, k)\n",
    "        print(centroids)\n",
    "        \n",
    "    else:\n",
    "        #DBSCAN\n",
    "        \n",
    "        num_disagree, dictionary = DBSCAN_main(X)\n",
    "\n",
    "#         if num_disagree == 0:\n",
    "#             print('PASS - All labels match!')\n",
    "\n",
    "#             X= X_population\n",
    "#             #uncomment\n",
    "#             #X = StandardScaler().fit_transform(X)\n",
    "#             #my_labels = MyDBSCAN(X, eps=0.3, MinPts=10)\n",
    "\n",
    "#         else:\n",
    "        print('FAIL -', num_disagree, 'labels don\\'t match.')\n",
    "\n",
    "        KMeans_cost, DBSCAN_cost, cache_stats_kmeans, cache_stats_dbscan = cost_analysis(len(X), dictionary, k, maxiter)\n",
    "        final_choice(KMeans_cost, DBSCAN_cost, X_population, k, maxiter)\n",
    "\n",
    "            \n",
    "        #accuracy_dbscan = float(count)/len(skl_labels)\n",
    "        #print(accuracy_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
